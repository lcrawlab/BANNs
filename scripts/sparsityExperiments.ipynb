{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Creating Deterministically Sparse Networks\n",
    "Pinar Demetci, Lorin Crawford Lab 2019, Brown University\n",
    "\n",
    "In our neural network model, the connections of the neural network is determined by biological annotations. This means, we construct deterministically sparse feedforward neural networks here. As a result of some brainstorming, the following are the ideas we came up with for achieving this. The last one (#5) is the method we currently use in our model, and we try to show here that it indeed works after discussing a few other options that one might think are possible but are not ideal.\n",
    "\n",
    "**1) Creating dense subnetworks and then joining them all together into one final sparsely-connected network**\n",
    "> $\\;\\;\\;\\;\\;$ This option does not work in the case of overlapping connections i.e. a SNP falls into multiple genetic annotations (due to overlap in annotations) or a gene is part of multiple pathways, which can indeed be the case, biologically:\n",
    "    <img src=\"images/sparsity_subnetwork.png\">  \n",
    "$\\;\\;\\;\\;\\;$ This image displays the idea in a tiny example of a neural network. In the case of no overlaps in connections, the outputs of <font color=maroon> subnetwork#1 </font> amd <font color=blue>subnetwork#2</font> can be concatenated and used as the input of <font color=green>subnetwork#3</font>. This connects the dense subnetworks together into one sparsely connected network and the backpropagation happens through the subnetworks. Example code at the end of this section.  \n",
    "However, in the case of overlapping connections (i.e. a node is connected to more than one subnetwork), we need to include the node with these connections in more than one subnetwork. In the example figure above, this would be the purple node, \"x3\". This leads to including a single input point multiple times in the resulting network as if there were multiple inputs with the same value, which introduces a bias/inaccuracy into the model.  \n",
    "So this approach will not work for our purposes, where genes might participate in multiple pathways (e.g. ....) or multiple SNPs might fall in the neighborhood of multiple genetic elements based on the genomic annotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Deterministically initializing weights of the sparse layers as 0s and 1s, freezing these layers, and only training the final layer variables:** \n",
    "> $\\;\\;\\;\\;\\;$ This ensures that except for the final layer, the weights are either 1 or 0, initialized based on biological annotations and are never changed due to freezing (the \"frozen\" layers are not trained). Illustration for this is below:  \n",
    "<img src=\"images/sparsity_freeze.png\"> \n",
    "The way to \"freeze\" a layer is to set that layer to be untrainable as demonstrated in the code snippet below.  \n",
    "But this method means we do not allow for training the weights of connections between SNPs and genes (or genes and pathways, as well, if we are carrying out pathway-based inference). So, no training of effect sizes of (or contribution of) SNPs to genes. This limits the model a little bit. It might not be a good idea biologically, because we know from empirical studies that not all SNPs affect gene expression at the same level. So, this is a oossible solution, but not really a desirable one. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    512\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    978\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(\"MatMul_1/b:0\", shape=(4, 2), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dc05faf08f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     [tf.constant(0.0), tf.Variable(1.0)]]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlayer1_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2453\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2455\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5331\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5332\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5333\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   5334\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5335\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    545\u001b[0m                   \u001b[0;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[0;32m--> 547\u001b[0;31m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'b' of 'MatMul' Op has type float32 that does not match type float64 of argument 'a'."
     ]
    }
   ],
   "source": [
    "\n",
    "W1=[[tf.Variable(1.0), tf.constant(0.0)],\n",
    "    [tf.Variable(1.0), tf.constant(0.0)],\n",
    "    [tf.Variable(1.0), tf.Variable(1.0)],\n",
    "    [tf.constant(0.0), tf.Variable(1.0)]]\n",
    "\n",
    "layer1_output=tf.matmul(X,W1)\n",
    "print(layer1_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Initializing weight matrices as a mix of \"variables\" and \"constants\" so only some of them are trained in the backpropagation**  \n",
    "> $\\;\\;\\;\\;\\;$ This is basically the same idea as #2, except instead of freezing all the weights in a given layer, we freeze only some of them by declaring them \"constants\". So, our weight matrix will have a mix of variables (trainable values) and constants (untrainable values).  \n",
    "While this works, it is not a great solution, because:\n",
    "\n",
    "**4) Overwriting the \"Dropout Layer\" of tensorflow to carry-out deterministic dropouts.**  \n",
    "> $\\;\\;\\;\\;\\;$ When people create sparse feed-forward neural networks with tensorflow or keras, they use the \"Dropout\" method. This method randomly drops connections after the user defines what percent of connections in a layer they want to keep. So, one might presume that a good way to carry out deterministically connected layers is to modify the Dropout method, such that we can specify which connections to drop in the input. The source code of Dropout can be found  here:  \n",
    "And the paper that first introduced the idea of dropout for regularizing neural networks is:  \n",
    "<a href=\"http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf\">Shrivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. \"Dropout: A simple way to prevent neural networks from overfitting‚Äù, JMLR 2014.</a>\n",
    "As can be seen in both the source code as well as the publication that introduces dropout, this method drops nodes, not connections (image below is from the paper):\n",
    "<img src=\"images/sparsity_dropout.png\">\n",
    "However, in our case, every SNP will be connected to at least one gene and all the genes that have some SNP information will be connected to at least one pathway (or will be connected to an \"unannotated\" node). What we want is to keep every node but drop-out connections, not drop-out connections by dropping out neurons. So, \n",
    "\n",
    "**5) Creating a \"mask matrix\" of 0s and 1s to multiply weights with in every iteration of forward pass, so those connections are dropped (\"zeroed-out\") and don't contribute to the output or gradient calculations.**  \n",
    "> $\\;\\;\\;\\;\\;$This does work quite smoothly. It is demonstrated below (by keeping track of weight updates and gradients). This is basically the same idea behind drop-out but the masks multiply the weight matrix instead of the input matrix. In weight matrix, every row corresponds to a \"SNP\" and every column corresponds to a \"gene\" for the first hidden layer. For the second hidden layer (if we are carry out pathway inference), the rows of the weight matrix will correspond to genes and the columns will correspond to pathways. Therefore, in the first hidden layer, for example, setting an element of the weight matrix to 0 means dropping the connection between the SNP that corresponds to the row value of that element and the gene that corresponds to the columnn value. When the weight is always multiplied by zero, whatever the value of the weight is, the output generated by this element will be the same, meaning the  gradient is 0. If  the gradient is zero,  then it will not be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
